{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression : Questions"
      ],
      "metadata": {
        "id": "kdCtJDQWyftO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a statistical method used to understand the relationship between two continuous variables - one independent (input) and one dependent (output). It tries to model this relationship by fitting a straight line (called a regression line) through the data points.\n",
        "  - In simple terms, it's like trying to draw the best possible straight line through a scatterplot of data so that we can predict one variable based on the other. For example, if we have data on the number of hours studied (independent variable) and exam scores (dependent variable), simple linear regression helps us predict exam scores based on the number of hours someone studies.\n",
        "  - The formula for this line is:\n",
        "$$\n",
        "y = mx + c\n",
        "$$\n",
        "  - Where :\n",
        "      - y is the predicted value (dependent variable),\n",
        "      - x is the input value (independent variable),\n",
        "      - m is the slope of the line (shows how much y changes for a unit change in x),\n",
        "      - c is the intercept (the value of y when x is 0)."
      ],
      "metadata": {
        "id": "NwL1nFo6yjGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "  - Simple Linear Regression works best when certain assumptions about the data are met. These assumptions help ensure that the model gives accurate and reliable predictions. The key assumptions are :\n",
        "  1. Linearity :\n",
        "      - The relationship between the independent variable (x) and the dependent variable (y) should be linear - meaning, the data should generally follow a straight-line pattern.\n",
        "  2. Independence of Errors :\n",
        "       - The residuals (i.e., the differences between the actual and predicted values) should be independent. This means that one error shouldn't influence another - especially important when data is collected over time.\n",
        "  3. Homoscedasticity (Constant Variance of Errors) :\n",
        "       - The variance of the errors should remain constant across all values of the independent variable. In other words, the spread of the residuals should look consistent across the line, not wider or narrower in any specific area.\n",
        "  4. Normality of Errors :\n",
        "       - The residuals should be approximately normally distributed. This assumption becomes especially important when we want to calculate confidence intervals or perform hypothesis testing.\n",
        "  5. No Multicollinearity :\n",
        "       - While this isn't directly about simple linear regression (more important in multiple regression), it means the independent variables shouldn't be highly correlated with each other. Since simple regression only uses one independent variable, this usually isn't an issue here."
      ],
      "metadata": {
        "id": "J4e5KAGr1DI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "  - In the equation Y = mX+c, which represents a straight line in simple linear regression :\n",
        "      - m is the slope of the line.\n",
        "  - This slope tells us how much the dependent variable (Y) is expected to change when the independent variable (X) increases by 1 unit.\n",
        "  - Example : If m = 2, it means :\"For every 1 unit increase in X, Y increases by 2 units.\"\n",
        "  - If m is negative, the line goes downward and it means Y decreases as X increases.\n",
        "  - In simple terms, m shows the strength and direction of the relationship between X and Y.\n"
      ],
      "metadata": {
        "id": "Y8CyMP471ZGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "  - In the equation Y = mX+c, the c is the intercept, also known as the y-intercept.\n",
        "  - It represents the value of Y when X is 0. In other words, it's where the line crosses the Y-axis on a graph.\n",
        "  - Example : If we are predicting someone's salary based on years of experience and c = 30,000 that means - \"When the person has 0 years of experience, the predicted salary is 30,000.\"\n",
        "  - So, the intercept gives us a starting point for the prediction - it's the baseline value of Y when the independent variable X is not present."
      ],
      "metadata": {
        "id": "rRAsK0j74HA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "  - The slope m tells us how much the dependent variable (Y) changes when the independent variable (X) increases by one unit. It's calculated using a formula based on the data points.\n",
        "  - The formula is :\n",
        "$$\n",
        "m = \\frac{y_2 - y_1}{x_2 - x_1}\n",
        "$$\n",
        "\n",
        "  - This formula calculates the slope of a straight line when we have exactly two data points : $(x_1, y_1)$ and $(x_2, y_2)$.\n",
        "  - It gives us the rate of change between those two points, which is technically the slope between them.\n",
        "  - In Simple Linear Regression, we usually have many data points, not just two. The goal is to find the best-fitting line through all the data points - not just connect two.\n",
        "  - That's why we use the formula :\n",
        "\n",
        "$$\n",
        "m = \\frac{n \\sum (XY) - \\sum X \\sum Y}{n \\sum (X^2) - (\\sum X)^2}\n",
        "$$\n",
        "\n",
        "  - Where :\n",
        "      - n = number of data points\n",
        "      - ΣXY = sum of the product of each X and Y\n",
        "      - ΣX = sum of all X values\n",
        "      - ΣY = sum of all Y values\n",
        "      - ΣX² = sum of the squares of all X values\n",
        "  - This formula uses all the points and minimizes the overall error - not just the slope between two specific points.\n",
        "  - In simple words : The formula finds the best-fitting line by minimizing the total distance (error) between the actual Y values and the predicted Y values on the line. This distance is measured vertically - and that's what residuals represent.\n",
        "  - If we are using Python, we don't need to calculate this manually - libraries like scikit-learn or even NumPy can compute it for us."
      ],
      "metadata": {
        "id": "vR2cOv0343UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "  - The least squares method is used in simple linear regression to find the best-fitting straight line through a set of data points.\n",
        "  - Its main purpose is :\n",
        "      - To minimize the sum of the squares of the residuals - where a residual is the difference between an actual value and the predicted value from the line.\n",
        "  - Why we square the residuals :\n",
        "      - Squaring makes all the errors positive (so they don’t cancel each other out).\n",
        "      - It gives more weight to larger errors, helping the model focus on reducing big mistakes.\n",
        "  - In simple words :\n",
        "      - The least squares method helps us draw the line that fits the data as closely as possible, by making sure the overall prediction error is as small as it can be.\n",
        "      - This is the method that determines the best values for the slope (m) and intercept (c) in the equation :\n",
        "$$\n",
        "Y = mX + c\n",
        "$$"
      ],
      "metadata": {
        "id": "2ufVe8qM9LbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "  - The coefficient of determination, written as R² (R-squared), is a number that tells us how well our regression model explains the variability of the dependent variable (Y).\n",
        "  - In simple terms : R² shows us how much of the change in Y can be explained by the change in X.\n",
        "      - R² = 1 (or 100%) - Perfect fit : the model explains all the variation in the data.\n",
        "      - R² = 0 - The model explains none of the variation - it's no better than guessing the average.\n",
        "      - R² between 0 and 1 - The model explains some of the variation - the closer to 1, the better the model fits.\n",
        "  - Example : If R² = 0.85, it means :\n",
        "      - \"85% of the variation in the dependent variable can be explained by the independent variable.\"\n",
        "  - R² is a measure of how well the line fits the data and higher values usually mean better predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "wijz1ce6-13r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "  - Multiple Linear Regression is an extension of simple linear regression. Instead of using just one independent variable to predict the dependent variable, it uses two or more independent variables.\n",
        "  - In simple terms :\n",
        "      - It helps us understand how several factors together affect the outcome we're trying to predict.\n",
        "  - The formula looks like :\n",
        "$$\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n\n",
        "$$\n",
        "\n",
        "  - Where :\n",
        "      - Y is the dependent (target) variable\n",
        "      - X₁, X₂, ..., Xₙ are the independent (input) variables\n",
        "      - b₀ is the intercept (value of Y when all Xs are 0)\n",
        "      - b₁, b₂, ..., bₙ are the coefficients (slopes) showing how much Y changes with a 1-unit change in each X\n",
        "  - Example :\n",
        "      - If we are predicting a house price (Y) based on :\n",
        "          - square footage (X₁),\n",
        "          - number of bedrooms (X₂) and\n",
        "          - location rating (X₃),\n",
        "      - then multiple linear regression finds how all these factors together influence the house price."
      ],
      "metadata": {
        "id": "KrF9xGu0AygS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "  - The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "  - Simple Linear Regression :\n",
        "      - Uses only one independent variable.\n",
        "      - Example : Predicting salary based on years of experience.\n",
        "  - Multiple Linear Regression :\n",
        "      - Uses two or more independent variables.\n",
        "      - Example : Predicting salary based on years of experience, education level and location.\n",
        "  - In simple words :\n",
        "      - Simple linear regression finds a straight line in 2D space.\n",
        "      - Multiple linear regression fits a plane or hyperplane in higher dimensions.\n",
        "  - Both aim to predict the outcome (Y), but multiple linear regression can handle more complex situations by considering more input factors."
      ],
      "metadata": {
        "id": "Ty_bM7f_BUAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "  - Multiple Linear Regression works best when certain assumptions are met. These assumptions help ensure that the results are valid and the predictions are reliable.\n",
        "  - Key assumptions:\n",
        "  1. Linearity :\n",
        "      - The relationship between the dependent variable and each independent variable should be linear.\n",
        "  2. Independence of Errors :\n",
        "      - The residuals (errors) should be independent - one error shouldn't influence another. This is especially important for time-based data.\n",
        "  3. Homoscedasticity :\n",
        "      - The variance of the residuals should be constant across all levels of the independent variables. In simple terms, the spread of the errors should be roughly the same throughout the data.\n",
        "  4. Normality of Errors :\n",
        "      - The residuals should be approximately normally distributed, especially if we want to calculate confidence intervals or perform hypothesis tests.\n",
        "\n",
        "  5. No Multicollinearity :\n",
        "      - The independent variables should not be highly correlated with each other. If they are, it becomes hard to tell which variable is really affecting the dependent variable.\n",
        "  6. No Autocorrelation :\n",
        "      - This mostly applies to time series data. The residuals shouldn't show a pattern over time - they should be random.\n",
        "  - In short, these assumptions make sure the model is reliable and interpretable. If they are violated, the predictions might still come out, but the results could be misleading."
      ],
      "metadata": {
        "id": "_6R0vr5uEFJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity and how does it affect the results of a Multiple Linear Regression model?\n",
        "  - Heteroscedasticity refers to a situation where the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "  - In simple words : Normally, we expect the prediction errors (residuals) to be spread out evenly. But in heteroscedasticity, the spread of errors changes - it might get wider or narrower for certain values of X.\n",
        "  - Example :\n",
        "      - Imagine predicting house prices. For cheaper houses, the prediction errors might be small, but for expensive houses, the errors might be huge - that's heteroscedasticity.\n",
        "  - How it affects the model :\n",
        "      - It doesn't bias the regression coefficients (they're still correct on average).\n",
        "      - But it makes the standard errors unreliable, which can lead to :\n",
        "          - Incorrect confidence intervals\n",
        "          - Wrong p-values\n",
        "          - Misleading conclusions in hypothesis testing\n",
        "  - So, while our model might still give predictions, the interpretation of those results becomes less trustworthy.\n"
      ],
      "metadata": {
        "id": "Aa83647CFziw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "  - Multicollinearity means that two or more independent variables in our model are highly correlated with each other - they carry similar information. This can make our model unstable and hard to interpret.\n",
        "  - Ways to improve the model :\n",
        "  1. Remove highly correlated variables :\n",
        "      - Keep the variable that's more meaningful and drop the one that's too similar.\n",
        "  2. Combine correlated features :\n",
        "      - Create a new variable by averaging or combining the correlated features into one.\n",
        "\n",
        "  3. Use Principal Component Analysis (PCA) :\n",
        "      - PCA transforms our variables into a new set of uncorrelated features (called principal components).\n",
        "\n",
        "  4. Apply regularization techniques :\n",
        "      - Use models like Ridge Regression or Lasso Regression, which can handle multicollinearity better by penalizing large or unnecessary coefficients.\n",
        "\n",
        "  5. Check the Variance Inflation Factor (VIF) :\n",
        "      - Use VIF to detect multicollinearity. If a variable has a high VIF (usually > 5 or 10), it may need to be removed or transformed.\n",
        "  - Multicollinearity doesn't hurt predictions much, but it makes it hard to understand which variable is really affecting the outcome. Fixing it improves the clarity and stability of our model."
      ],
      "metadata": {
        "id": "W9FZzxzJHNgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "  - Categorical variables (like “Gender” or “Car Brand”) need to be converted into numbers before they can be used in regression models. Here are some common techniques :\n",
        "  1. One-Hot Encoding :    \n",
        "      - Converts each category into a separate binary (0 or 1) column.\n",
        "      - Example :\n",
        "          - \"Color\" : Red, Blue, Green\n",
        "          - Becomes :\n",
        "          - Color_Red, Color_Blue, Color_Green\n",
        "      - Used when categories have no natural order (nominal data).\n",
        "  2. Label Encoding :     \n",
        "      - Assigns a unique number to each category.\n",
        "      - Example : \"Low\" = 0, \"Medium\" = 1, \"High\" = 2\n",
        "      - Best for ordinal data (when categories have a clear order).\n",
        "  3. Binary Encoding / Hashing :    \n",
        "      - Used when there are many categories.\n",
        "      - These methods reduce the number of columns created compared to one-hot encoding, which can help with memory and performance.\n",
        "  - For regression models, label encoding is risky for non-ordered categories (like city names), because it may mislead the model into thinking there's an order."
      ],
      "metadata": {
        "id": "0OOzDy48MISt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "  - Interaction terms are used in multiple linear regression to capture the combined effect of two (or more) variables on the dependent variable - when their relationship isn't just additive.\n",
        "  - In simple words: Sometimes, the effect of one variable depends on the value of another variable. Interaction terms help the model understand that.\n",
        "  - Example :    \n",
        "      - Suppose we are predicting sales based on :\n",
        "          - Ad Spend (X₁)\n",
        "          - Season (X₂)\n",
        "      - If ads work much better during festive seasons than regular times, then the effect of Ad Spend depends on the Season. That's an interaction!\n",
        "  - We add a new variable:\n",
        "$$\n",
        "\\text{Interaction} = X₁ \\times X₂\n",
        "$$\n",
        "\n",
        "  - And the model becomes:\n",
        "\n",
        "$$\n",
        "Y = b₀ + b₁X₁ + b₂X₂ + b₃(X₁ \\times X₂)\n",
        "$$\n",
        "  - Why it's useful :\n",
        "      - It reveals more complex patterns in data.\n",
        "      - Without interaction terms, the model might miss important relationships between variables.\n"
      ],
      "metadata": {
        "id": "EWGdOo_eN9Jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "  - The intercept is the value of the dependent variable (Y) when all independent variables are zero.\n",
        "  - In Simple Linear Regression :\n",
        "      - There is only one independent variable (X).\n",
        "      - The intercept is the predicted value of Y when X = 0.\n",
        "      - Example: If we are predicting salary based on years of experience, the intercept is the estimated salary for someone with 0 years of experience.\n",
        "  - In Multiple Linear Regression :\n",
        "      - There are multiple independent variables (X₁, X₂, ..., Xₙ).\n",
        "      - The intercept is the predicted value of Y when all X variables are zero.\n",
        "      - This can be harder to interpret, especially if a value of 0 doesn't make sense for all variables (like height = 0 or age = 0).\n",
        "  - So, in simple regression, the intercept is easier to interpret. And in multiple regression, the interpretation is more complex and sometimes not meaningful - it depends on whether having all inputs equal to 0 is realistic in our context."
      ],
      "metadata": {
        "id": "XfRFacc3QBWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "  - The slope in regression analysis tells us how much the dependent variable (Y) changes when the independent variable (X) increases by one unit, assuming all other variables stay the same.\n",
        "  - Why it matters :\n",
        "      - It shows the strength and direction of the relationship between X and Y.\n",
        "          - A positive slope means Y increases as X increases.\n",
        "          - A negative slope means Y decreases as X increases.\n",
        "  - In predictions :\n",
        "      - The slope is what the model uses to adjust predictions based on changes in the input variable(s).\n",
        "      - Example (Simple Linear Regression) - If the slope is 3, it means :\n",
        "          - \"For every 1 unit increase in X, Y increases by 3 units.\"\n",
        "  - In Multiple Linear Regression :\n",
        "      - Each variable has its own slope (coefficient) and each one shows how that particular variable affects Y, while controlling for the others.\n",
        "  - The slope helps us understand how inputs drive the output and it's directly used to make predictions. It's one of the most important parts of the regression equation."
      ],
      "metadata": {
        "id": "_VcZPIt0RMqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "  - The intercept in a regression model is the predicted value of the dependent variable (Y) when all the independent variables (X) are equal to zero.\n",
        "  - Why it provides context :\n",
        "      - It gives us a starting point for the relationship - the base value of Y before any of the predictors have an effect.\n",
        "      - It helps complete the regression equation :\n",
        "$$\n",
        "  Y = b_0 + b_1X_1 + b_2X_2 + \\dots + b_nX_n\n",
        "$$\n",
        "\n",
        "  - where :\n",
        "      - b₀ is the intercept.\n",
        "\n",
        "  - Example - If we are predicting someone's salary based on experience and education :    \n",
        "      - The intercept might represent the estimated salary of someone with 0 years of experience and no education - even if that case doesn't happen in real life, it helps set the baseline for the model.\n",
        "  - Sometimes the intercept has no real-world meaning (like predicting height when age = 0), but it's still needed for accurate predictions.\n",
        "  - In other cases, it gives useful insights - like the minimum cost, base rate or fixed value when nothing else is influencing the outcome."
      ],
      "metadata": {
        "id": "kwuAcSwSSmBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "  - R² (R-squared) tells us how much of the variation in the dependent variable (Y) is explained by the independent variables (X). While it's useful, relying on R² alone can be misleading.\n",
        "  - Limitations of R² :\n",
        "  1. Doesn't tell us if the model is good :\n",
        "      - A high R² doesn't always mean the model is a good fit - it might just be overfitting the data.\n",
        "  2. Can increase with unnecessary variables :\n",
        "      - Adding more variables (even useless ones) will always increase or keep R² the same, making the model look better than it actually is.\n",
        "  3. Doesn't show if relationships are meaningful :\n",
        "      - It doesn't tell us whether the variables are statistically significant or just random noise.\n",
        "  4. Not useful for comparing different types of models :\n",
        "      - R² works well for linear models but isn't reliable for nonlinear or classification models.\n",
        "  5. Doesn't reflect prediction accuracy :\n",
        "      - A model can have a high R² and still make large prediction errors. Metrics like RMSE, MAE or Adjusted R² can give more insight.\n",
        "  - R² is helpful, but it should be used alongside other metrics to judge how well a model performs and how trustworthy it is.\n"
      ],
      "metadata": {
        "id": "pio_U7jEUG8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "  - A large standard error for a regression coefficient means there's a lot of uncertainty about the value of that coefficient - in other words, we're less confident in the estimated effect of that variable on the outcome.\n",
        "  - What it tells us :\n",
        "      - The coefficient might vary a lot if we collected a new sample of data.\n",
        "      - The predictor might not have a strong or consistent relationship with the dependent variable.\n",
        "      - It could also mean there's multicollinearity (correlation with other predictors) or too much noise in the data.\n",
        "      - Example :\n",
        "          - Suppose our model estimates that an additional year of education increases income by 10,000, but the standard error is 8,000.\n",
        "          - That's a large standard error - it means the true effect could be much smaller or even negative. So, we can't trust the coefficient very much.\n",
        "  - In short, a large standard error means the coefficient is not precise and we should be cautious when interpreting its effect. We may also want to check the p-value to see if it's statistically significant.  "
      ],
      "metadata": {
        "id": "Cp3EdHO-Z4FH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "  - Identifying Heteroscedasticity :\n",
        "      - In a residual plot (residuals on the y-axis, predicted values or independent variable on the x-axis), heteroscedasticity appears as a pattern in the spread of residuals :\n",
        "          - Look for a funnel shape — where the residuals start tightly clustered and then spread out (or vice versa).\n",
        "          - In a good model, residuals should appear randomly scattered with constant spread — no clear pattern.\n",
        "  - Why it's important to fix :\n",
        "      - It violates a key assumption of regression : constant variance of residuals (homoscedasticity).\n",
        "      - It makes the model's standard errors unreliable, which :\n",
        "          - Affects confidence intervals\n",
        "          - Makes hypothesis tests (like p-values) misleading\n",
        "      - Predictions might still work, but statistical inferences become questionable.\n",
        "  - So, Heteroscedasticity shows up as changing spread in residual plots. It's important to detect and correct it (e.g., by transforming variables or using robust regression methods) to make our model more trustworthy."
      ],
      "metadata": {
        "id": "6895_PH7bBIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "  - What it means :\n",
        "      - A high R² means the model explains a large portion of the variation in the dependent variable (Y).\n",
        "      - But a low adjusted R² suggests that some of the predictors are not actually useful - they may just be adding noise.\n",
        "  - If our model has a high R² but a low adjusted R², it usually means that we have included too many irrelevant or unhelpful variables in our model.\n",
        "  -  Why this happens :\n",
        "      - R² always increases when we add more variables - even if those variables are not meaningful.\n",
        "      - Adjusted R², on the other hand, penalizes the model for adding irrelevant variables.\n",
        "          - It increases only if the new variable improves the model significantly.\n",
        "          - If the variable doesn't help, adjusted R² will go down.\n",
        "  - So, if R² is high and Adjusted R² is low means :\n",
        "      - The model may be overfitting - it looks good on paper but includes unnecessary variables that don't actually improve prediction.\n",
        "  -  In simple terms, the model might look impressive (high R²), but adjusted R² is warning us that some features are just taking up space without helping (we should consider removing or re-evaluating those features.)."
      ],
      "metadata": {
        "id": "hCeAKqU9bwvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "  - Scaling variables means bringing them to a similar range or unit (like using standardization or normalization). While it's not always required for Multiple Linear Regression, it's very important in some situations.\n",
        "  - Why scaling matters :\n",
        "  1. Helps interpret coefficients fairly :\n",
        "      - Without scaling, variables with larger numeric ranges can dominate the model, even if they aren't more important.\n",
        "  2. Improves numerical stability :\n",
        "      - Models can struggle with large differences in scale, which may cause computational errors or unstable results.\n",
        "  3. Essential for regularization (Ridge, Lasso) :\n",
        "      - These models use penalties based on coefficient size. If features aren't scaled, the penalty is unfairly applied and the model may perform poorly.\n",
        "  4. Makes model training faster and more accurate :\n",
        "      - Gradient descent (used in some implementations) converges faster when features are on the same scale.\n",
        "  - In short, scaling ensures that all variables are treated equally by the model, especially when their units or ranges are very different."
      ],
      "metadata": {
        "id": "HC9bSp7CewlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "  - Polynomial regression is a type of regression where the relationship between the independent variable (X) and the dependent variable (Y) is modeled as a polynomial (i.e., a curved line), rather than a straight line like in linear regression.\n",
        "  - The equation looks like :\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "$$\n",
        "  - Here :\n",
        "      - $X, X^2, X^3, \\dots$ are the polynomial terms,\n",
        "      - $b_0, b_1, \\dots$ are the coefficients learned by the model.\n",
        "  - When to use it :\n",
        "      - When the data shows a non-linear pattern that a straight line can't capture.\n",
        "      - For example, if the data curves upward or downward, a polynomial regression can fit that shape better than linear regression.\n",
        "  - Even though it models non-linear relationships, polynomial regression is still considered a form of linear regression - because it's linear in terms of the coefficients."
      ],
      "metadata": {
        "id": "IeAE-BZUf2qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "  - Linear Regression :\n",
        "      - Models the relationship between X and Y as a straight line.\n",
        "      - Equation :\n",
        "$$\n",
        "Y = b_0 + b_1X\n",
        "$$\n",
        "      - Assumes a linear relationship between input and output.\n",
        "      - Cannot capture curves or bends in the data.\n",
        "\n",
        "  - Polynomial Regression :\n",
        "      - Models the relationship using higher powers of X (like $X^2, X^3$, etc.).\n",
        "      - Equation example :\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots\n",
        "$$\n",
        "      - Can fit non-linear (curved) patterns in the data.\n",
        "      - The more terms (higher degree), the more complex the curve.\n",
        "  - Polynomial regression can model curved trends; linear regression cannot.\n",
        "  - Both are considered linear models in terms of their coefficients (not the shape of the curve).\n",
        "  - Polynomial regression is more flexible but can overfit if the degree is too high.\n"
      ],
      "metadata": {
        "id": "9-xPdXddhEjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "  - When the relationship between the independent variable (X) and the dependent variable (Y) is non-linear.\n",
        "  -  When a straight line (linear regression) doesn't fit the data well and we notice curves or bends in a scatter plot.\n",
        "  - When we want to model patterns like :\n",
        "      - U-shaped or inverted U-shaped trends.\n",
        "      - Exponential-like or wave-like growth/decline.\n",
        "  - In real-world scenarios such as:\n",
        "      - Growth curves (e.g., population or sales growth)\n",
        "      - Physics/engineering data with curved motion\n",
        "      - Economics (e.g., diminishing returns or cost curves)\n",
        "- Use polynomial regression only when justified by the data.\n",
        "- Higher-degree polynomials can lead to overfitting, especially if we go beyond what the pattern actually shows.\n"
      ],
      "metadata": {
        "id": "i4FLxc8nihlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "  - The general form of a polynomial regression equation is:\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\dots + b_nX^n\n",
        "$$\n",
        "  - Where :\n",
        "      - $Y$ = predicted (dependent) variable\n",
        "      - $X$ = independent variable\n",
        "      - $b_0$ = intercept (constant term)\n",
        "      - $b_1, b_2, ..., b_n$ = coefficients for each power of $X$\n",
        "      - $n$ = degree of the polynomial (e.g., 2 for quadratic, 3 for cubic)\n",
        "  - Example (for a 3rd-degree polynomial) :\n",
        "$$\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3\n",
        "$$\n",
        "  - This models a cubic relationship between X and Y.\n"
      ],
      "metadata": {
        "id": "KHzZ4D8CnoFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "  - Yes, polynomial regression can be extended to multiple variables - this is known as multivariate polynomial regression.\n",
        "  - Key Points :\n",
        "      - We can apply polynomial terms to each independent variable, like :\n",
        "$$\n",
        "Y = b_0 + b_1X_1 + b_2X_1^2 + b_3X_2 + b_4X_2^2 + \\dots\n",
        "$$\n",
        "      - We can also include interaction terms, such as:\n",
        "$$\n",
        "b_5X_1X_2\n",
        "$$\n",
        "      to capture the combined effect of two variables.\n",
        "      - This helps model complex, curved relationships involving more than one feature.\n",
        "  - The number of terms grows very quickly, making the model more complex.\n",
        "  - It may lead to overfitting if not controlled properly.\n"
      ],
      "metadata": {
        "id": "7C4jEmB6ofjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "  - Overfitting :\n",
        "      - Higher-degree polynomials can fit the training data too well, capturing noise instead of the true pattern.\n",
        "  - Poor extrapolation :\n",
        "      - Predictions outside the range of the data (extrapolation) can become wildly inaccurate, especially with high-degree curves.\n",
        "  - Computationally expensive :\n",
        "      - As the degree increases, the number of terms grows, making the model more complex and harder to compute.\n",
        "  - Hard to interpret :\n",
        "      - Coefficients in higher-degree models become difficult to understand or explain in real-world terms.\n",
        "  - Sensitive to outliers :\n",
        "      - Polynomial regression is more likely to be distorted by extreme values in the data.\n",
        "  - Multicollinearity risk :\n",
        "      - Including powers of the same variable (e.g., X, X^2, X^3) can lead to high correlation between features, making the model unstable.\n"
      ],
      "metadata": {
        "id": "HhwCsPUHpsc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "  - Common methods to choose the right polynomial degree :\n",
        "      - R² (Coefficient of Determination) :\n",
        "          - Measures how well the model explains the variance in the data.\n",
        "          - But beware : R² always increases with higher degree, even if the model isn't better.\n",
        "      - Adjusted R² :\n",
        "          - More reliable than R²; it penalizes unnecessary complexity by adjusting for the number of predictors.\n",
        "      - Cross-validation (like k-fold CV) :\n",
        "          - Helps assess how well the model generalizes to new data, not just training data.\n",
        "          - Useful for detecting overfitting in high-degree polynomials.\n",
        "      - Mean Squared Error (MSE) / Root Mean Squared Error (RMSE) :\n",
        "          - Lower values indicate better fit.\n",
        "          - Compare these on validation or test data to avoid being misled by training performance.\n",
        "      - AIC / BIC (Akaike or Bayesian Information Criterion) :\n",
        "          - These balance model fit and complexity - lower values are better.\n",
        "          - Useful when comparing models with different numbers of terms.\n",
        "  - In short, use a combination of adjusted R², cross-validation and error metrics to pick the polynomial degree that fits well without overfitting."
      ],
      "metadata": {
        "id": "ngT7ymtFrJOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "  - Visualization helps us understand, validate and communicate our polynomial regression model effectively.\n",
        "  - Why visualization is important in polynomial regression :\n",
        "      - Reveals the shape of the relationship :\n",
        "          - Helps us see how well the curve fits the data, especially for non-linear patterns.\n",
        "      - Detects overfitting or underfitting :\n",
        "          - A curve that's too wiggly might be overfitting; a too-simple line might miss important trends.\n",
        "      - Helps identify outliers or unusual points :\n",
        "          - Visual plots can reveal data points that strongly affect the curve's shape.\n",
        "      - Compare different degrees :\n",
        "          - Easily see how a degree 2 (quadratic) vs. degree 3 (cubic) model performs.\n",
        "      - Improves interpretability :    \n",
        "          - Makes it easier to explain our model to others (especially non-technical stakeholders).\n",
        "      - Validates model assumptions :\n",
        "          - Plotting residuals and the fitted curve helps ensure the model behaves as expected."
      ],
      "metadata": {
        "id": "WyvUKl5FsgZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?"
      ],
      "metadata": {
        "id": "vfs6aK-BuFrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare our data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 6, 14, 28, 45])\n",
        "\n",
        "# Transform features to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the linear regression model on the transformed data\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "print(\"Predicted values:\", y_pred)\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "\n",
        "# Visualize the result\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "TuLigGGVNTc8",
        "outputId": "a773c6ec-4499-4b97-d039-a4e52d64a6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values: [ 1.97142857  5.91428571 14.42857143 27.51428571 45.17142857]\n",
            "Coefficients: [ 0.         -2.91428571  2.28571429]\n",
            "Intercept: 2.5999999999999837\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}